{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuCH+mcg3zs/4hm3g/VKkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyelyra/Mid-Term-Task/blob/main/Mid-Term-Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama: Anyelyra Kantata\n",
        "\n",
        "NPM: 2206048625\n",
        "\n"
      ],
      "metadata": {
        "id": "bdYCq-K5rCTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library yang Dibutuhkan"
      ],
      "metadata": {
        "id": "PRPQFoVU2jDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XofSmCdO2kNg",
        "outputId": "842001e7-5687-4c82-e51f-407106c49cab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Transformers`: Library yang menyediakan berbagai model transformer pre-trained, termasuk BERT, yang bisa digunakan untuk tugas NLP seperti klasifikasi teks.\n",
        "\n",
        "`PyTorch`: Framework deep learning yang digunakan untuk memodelkan neural network dalam model BERT.\n",
        "\n",
        "`sklearn`: Menyediakan metrik evaluasi."
      ],
      "metadata": {
        "id": "dktKqkg45NTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapkan Dataset"
      ],
      "metadata": {
        "id": "wvG7MUtr6bB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita akan membuat dataset contoh, dengan beberapa teks dan label (1 untuk teks positif, 0 untuk negatif). Setelah itu, kita tokenisasi teks dan ubah menjadi tensor agar bisa diolah oleh model BERT."
      ],
      "metadata": {
        "id": "EcM5zlpr7gCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"I feel happy today!\", 1),\n",
        "    (\"This is a sad story.\", 0),\n",
        "    (\"Everything is great in my life.\", 1),\n",
        "    (\"I am feeling terrible right now.\", 0),\n",
        "    (\"I am so excited about the concert!\", 1),\n",
        "    (\"Today was a boring day.\", 0),\n",
        "    (\"What a wonderful surprise!\", 1),\n",
        "    (\"That was an awful experience.\", 0),\n",
        "    (\"The movie was amazing!\", 1),\n",
        "    (\"It was a disappointing match.\", 0),\n",
        "    (\"I feel joyful after hearing the news.\", 1),\n",
        "    (\"The outcome was dismal.\", 0),\n",
        "    (\"I am thrilled to start my new job!\", 1),\n",
        "    (\"This food tastes bad.\", 0),\n",
        "    (\"I'm delighted to meet you.\", 1),\n",
        "    (\"This place looks dreadful.\", 0),\n",
        "    (\"What a splendid idea!\", 1),\n",
        "    (\"It was a regrettable decision.\", 0),\n",
        "    (\"I have a fantastic feeling about this!\", 1),\n",
        "    (\"The report was lackluster.\", 0),\n",
        "    (\"I am very optimistic about the future.\", 1),\n",
        "    (\"It was an irritating situation.\", 0),\n",
        "    (\"Feeling great after a long workout!\", 1),\n",
        "    (\"This weather is not pleasant.\", 0),\n",
        "    (\"I'm so grateful for my friends!\", 1),\n",
        "    (\"The service was unacceptable.\", 0),\n",
        "    (\"I feel rejuvenated after my vacation.\", 1),\n",
        "    (\"That was a horrible mistake.\", 0),\n",
        "    (\"I am looking forward to the weekend!\", 1),\n",
        "    (\"I cannot stand this traffic.\", 0),\n",
        "    (\"It's such a lovely day outside.\", 1),\n",
        "    (\"The lecture was uninteresting.\", 0),\n",
        "    (\"I'm ecstatic about the upcoming event!\", 1),\n",
        "    (\"That was a miserable experience.\", 0),\n",
        "    (\"I feel so grateful for everything I have.\", 1),\n",
        "    (\"It was a catastrophic failure.\", 0),\n",
        "    (\"I'm excited to travel again.\", 1),\n",
        "    (\"This project is going poorly.\", 0),\n",
        "    (\"I am so proud of my accomplishments.\", 1),\n",
        "    (\"That was a lackluster performance.\", 0),\n",
        "    (\"I feel confident about my skills.\", 1),\n",
        "    (\"This product is disappointing.\", 0),\n",
        "    (\"I am overjoyed with the results!\", 1),\n",
        "    (\"The response was underwhelming.\", 0),\n",
        "    (\"I feel so inspired right now.\", 1),\n",
        "    (\"It was a disheartening situation.\", 0),\n",
        "    (\"What an encouraging atmosphere!\", 1),\n",
        "    (\"I feel trapped in this situation.\", 0),\n",
        "    (\"Today has been so refreshing!\", 1),\n",
        "    (\"This is a terrible plan.\", 0),\n",
        "    (\"I am really pumped for the game!\", 1),\n",
        "    (\"That was a discouraging comment.\", 0),\n",
        "    (\"I feel a sense of peace today.\", 1),\n",
        "    (\"That was a stressful day.\", 0),\n",
        "    (\"I had a fantastic day!\", 1),\n",
        "    (\"This is the worst experience ever.\", 0),\n",
        "    (\"I'm feeling incredibly positive today!\", 1),\n",
        "    (\"That was a disappointing outcome.\", 0),\n",
        "    (\"I'm thrilled with the results!\", 1),\n",
        "    (\"This situation is quite frustrating.\", 0),\n",
        "    (\"I love spending time with my family!\", 1),\n",
        "    (\"I can't believe how bad this is.\", 0),\n",
        "    (\"What a great surprise!\", 1),\n",
        "    (\"This product is not what I expected.\", 0),\n",
        "    (\"I'm so happy to be here!\", 1),\n",
        "    (\"The food was awful.\", 0),\n",
        "    (\"I can't wait for the concert!\", 1),\n",
        "    (\"This has been a terrible week.\", 0),\n",
        "    (\"I'm so grateful for my health!\", 1),\n",
        "    (\"I am unhappy with my purchase.\", 0),\n",
        "    (\"Today is a wonderful day!\", 1),\n",
        "    (\"I feel so down right now.\", 0),\n",
        "    (\"I'm excited about the future!\", 1),\n",
        "    (\"That was a disappointing performance.\", 0),\n",
        "    (\"I'm feeling motivated and inspired!\", 1),\n",
        "    (\"I hate this weather.\", 0),\n",
        "    (\"What an amazing opportunity!\", 1),\n",
        "    (\"I'm frustrated with the service.\", 0),\n",
        "    (\"I feel blessed to have such great friends.\", 1),\n",
        "    (\"This event was poorly organized.\", 0),\n",
        "    (\"I'm looking forward to the holidays!\", 1),\n",
        "    (\"This is a horrible situation.\", 0),\n",
        "    (\"I feel fulfilled after finishing my project!\", 1),\n",
        "    (\"That was a dull movie.\", 0),\n",
        "    (\"I'm thrilled to learn something new!\", 1),\n",
        "    (\"That was a big mistake.\", 0),\n",
        "    (\"I'm feeling so hopeful today!\", 1),\n",
        "    (\"This is a major setback.\", 0),\n",
        "    (\"I'm enjoying every moment!\", 1),\n",
        "    (\"I feel completely let down.\", 0),\n",
        "    (\"I'm ecstatic about the results!\", 1),\n",
        "    (\"This has been an exhausting day.\", 0),\n",
        "    (\"I'm so proud of what I've accomplished!\", 1),\n",
        "    (\"That was a terrible decision.\", 0),\n",
        "    (\"I'm feeling great about my progress!\", 1),\n",
        "    (\"This is such a letdown.\", 0),\n",
        "    (\"I'm so happy with my choices!\", 1),\n",
        "    (\"This project is a disaster.\", 0),\n",
        "    (\"What a delightful surprise!\", 1),\n",
        "    (\"I'm tired of this constant negativity.\", 0),\n",
        "    (\"I feel like celebrating!\", 1),\n",
        "    (\"That was a very sad story.\", 0),\n",
        "    (\"I feel so grateful for my life!\", 1),\n",
        "    (\"This was a frustrating experience.\", 0),\n",
        "    (\"I'm ready to take on the world!\", 1),\n",
        "    (\"That was a waste of time.\", 0),\n",
        "]"
      ],
      "metadata": {
        "id": "lVFylKeM6czr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konversi Data menjadi TensorDataset"
      ],
      "metadata": {
        "id": "hd9szAxjE0SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts, labels = zip(*data)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "dataset = CustomDataset(texts, labels)"
      ],
      "metadata": {
        "id": "ofAinHYoE4qH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membagi Data menjadi Latih dan Validasi"
      ],
      "metadata": {
        "id": "nq1EYKV-A4hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "3KgvJMPTA_uQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat BERT Tokenizer"
      ],
      "metadata": {
        "id": "w2hgDMeC74qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita perlu membuat `DataLoader` agar data bisa diproses dalam batch selama pelatihan.\n"
      ],
      "metadata": {
        "id": "kF5ltx808byl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_data(dataset):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for text, label in dataset:\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        input_ids.append(encoding['input_ids'])\n",
        "        attention_masks.append(encoding['attention_mask'])\n",
        "        labels.append(label)\n",
        "\n",
        "    return torch.cat(input_ids), torch.cat(attention_masks), torch.tensor(labels)\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_dataset)\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6lnY1d_8Gnu",
        "outputId": "c2ea617d-d654-4c49-a827-b9106b41a62d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat DataLoaders untuk Latih dan Validasi\n",
        "\n"
      ],
      "metadata": {
        "id": "MTn_YUDPFTMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "O-XrM7GfFX5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "BU7P3trY8h5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Set the model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "# Set a learning rate scheduler to help with fine-tuning\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fimd92VD8jxg",
        "outputId": "dc606795-42ad-4f00-bd1f-c24ce09c6857"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FzKMezQa8wSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhcX8FBI8y7j",
        "outputId": "573e38d1-bf2d-4e87-b841-aa48e950a9ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:04<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Accuracy: 0.9091, Precision: 1.0000, Recall: 0.8000, F1 Score: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:01<00:00,  5.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Accuracy: 0.9545, Precision: 0.9091, Recall: 1.0000, F1 Score: 0.9524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:01<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Accuracy: 0.9545, Precision: 0.9091, Recall: 1.0000, F1 Score: 0.9524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrik Evaluasi Keseluruhan"
      ],
      "metadata": {
        "id": "pm3NZ0jDIuPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_accuracy = accuracy_score(true_labels, predictions)\n",
        "overall_precision = precision_score(true_labels, predictions)\n",
        "overall_recall = recall_score(true_labels, predictions)\n",
        "overall_f1 = f1_score(true_labels, predictions)"
      ],
      "metadata": {
        "id": "RTUEFGJIItbI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
        "print(f'Overall Precision: {overall_precision:.4f}')\n",
        "print(f'Overall Recall: {overall_recall:.4f}')\n",
        "print(f'Overall F1 Score: {overall_f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzk7Ho6_IzuV",
        "outputId": "7ee2790c-5731-4fbf-b139-a284bf310a46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.9545\n",
            "Overall Precision: 0.9091\n",
            "Overall Recall: 1.0000\n",
            "Overall F1 Score: 0.9524\n"
          ]
        }
      ]
    }
  ]
}